2024-08-23 15:59:51,025 - Log file for this run: /root/Summer_Project/ai8x-training/logs/2024.08.23-155951/2024.08.23-155951.log
2024-08-23 15:59:51,025 - The open file limit is 1024. Please raise the limit (see documentation).
2024-08-23 15:59:51,025 - Configuring device: MAX78000, simulate=False.
2024-08-23 15:59:51,228 - => loading checkpoint logs/2024.08.23-155107/checkpoint.pth.tar
2024-08-23 15:59:51,232 - => Checkpoint contents:
+----------------------+-------------+----------+
| Key                  | Type        | Value    |
|----------------------+-------------+----------|
| arch                 | str         | mnist_75 |
| compression_sched    | dict        |          |
| epoch                | int         | 2        |
| extras               | dict        |          |
| optimizer_state_dict | dict        |          |
| optimizer_type       | type        | SGD      |
| state_dict           | OrderedDict |          |
+----------------------+-------------+----------+

2024-08-23 15:59:51,232 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  2      |
| best_mAP     | int    |  0      |
| best_top1    | float  | 92.9167 |
| current_mAP  | int    |  0      |
| current_top1 | float  | 92.9167 |
+--------------+--------+---------+

2024-08-23 15:59:51,233 - Loaded compression schedule from checkpoint (epoch 2)
2024-08-23 15:59:51,242 - => loaded 'state_dict' from checkpoint 'logs/2024.08.23-155107/checkpoint.pth.tar'
2024-08-23 15:59:51,277 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-08-23 15:59:51,277 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}
2024-08-23 15:59:51,319 - Reading compression schedule from: policies/schedule.yaml
2024-08-23 15:59:51,861 - torch.compile() successful, mode=default, cache limit=8
2024-08-23 15:59:51,861 - Dataset sizes:
	training=54000
	validation=6000
	test=10000
2024-08-23 15:59:51,862 - 

2024-08-23 15:59:51,862 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:59:53,216 - /root/Summer_Project/ai8x-training/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(

2024-08-23 15:59:57,823 - Epoch: [0][   10/  211]    Overall Loss 2.008215    Objective Loss 2.008215                                        LR 0.100000    Time 0.596023    
2024-08-23 15:59:57,938 - Epoch: [0][   20/  211]    Overall Loss 1.725590    Objective Loss 1.725590                                        LR 0.100000    Time 0.303775    
2024-08-23 15:59:58,041 - Epoch: [0][   30/  211]    Overall Loss 1.567319    Objective Loss 1.567319                                        LR 0.100000    Time 0.205930    
2024-08-23 15:59:58,161 - Epoch: [0][   40/  211]    Overall Loss 1.472158    Objective Loss 1.472158                                        LR 0.100000    Time 0.157436    
2024-08-23 15:59:58,280 - Epoch: [0][   50/  211]    Overall Loss 1.403310    Objective Loss 1.403310                                        LR 0.100000    Time 0.128323    
2024-08-23 15:59:58,408 - Epoch: [0][   60/  211]    Overall Loss 1.353677    Objective Loss 1.353677                                        LR 0.100000    Time 0.109056    
2024-08-23 15:59:58,526 - Epoch: [0][   70/  211]    Overall Loss 1.313997    Objective Loss 1.313997                                        LR 0.100000    Time 0.095157    
2024-08-23 15:59:58,654 - Epoch: [0][   80/  211]    Overall Loss 1.280861    Objective Loss 1.280861                                        LR 0.100000    Time 0.084865    
2024-08-23 15:59:58,773 - Epoch: [0][   90/  211]    Overall Loss 1.257614    Objective Loss 1.257614                                        LR 0.100000    Time 0.076754    
2024-08-23 15:59:58,923 - Epoch: [0][  100/  211]    Overall Loss 1.236097    Objective Loss 1.236097                                        LR 0.100000    Time 0.070571    
2024-08-23 15:59:59,066 - Epoch: [0][  110/  211]    Overall Loss 1.218494    Objective Loss 1.218494                                        LR 0.100000    Time 0.065447    
2024-08-23 15:59:59,198 - Epoch: [0][  120/  211]    Overall Loss 1.203208    Objective Loss 1.203208                                        LR 0.100000    Time 0.061091    
2024-08-23 15:59:59,315 - Epoch: [0][  130/  211]    Overall Loss 1.189240    Objective Loss 1.189240                                        LR 0.100000    Time 0.057289    
2024-08-23 15:59:59,462 - Epoch: [0][  140/  211]    Overall Loss 1.176230    Objective Loss 1.176230                                        LR 0.100000    Time 0.054245    
2024-08-23 15:59:59,594 - Epoch: [0][  150/  211]    Overall Loss 1.164807    Objective Loss 1.164807                                        LR 0.100000    Time 0.051508    
2024-08-23 15:59:59,722 - Epoch: [0][  160/  211]    Overall Loss 1.153758    Objective Loss 1.153758                                        LR 0.100000    Time 0.049082    
2024-08-23 15:59:59,836 - Epoch: [0][  170/  211]    Overall Loss 1.146286    Objective Loss 1.146286                                        LR 0.100000    Time 0.046867    
2024-08-23 15:59:59,960 - Epoch: [0][  180/  211]    Overall Loss 1.139338    Objective Loss 1.139338                                        LR 0.100000    Time 0.044949    
2024-08-23 16:00:00,087 - Epoch: [0][  190/  211]    Overall Loss 1.131414    Objective Loss 1.131414                                        LR 0.100000    Time 0.043251    
2024-08-23 16:00:00,214 - Epoch: [0][  200/  211]    Overall Loss 1.125474    Objective Loss 1.125474                                        LR 0.100000    Time 0.041721    
2024-08-23 16:00:00,334 - Epoch: [0][  210/  211]    Overall Loss 1.119574    Objective Loss 1.119574    Top1 90.625000    Top5 99.609375    LR 0.100000    Time 0.040304    
2024-08-23 16:00:04,814 - Epoch: [0][  211/  211]    Overall Loss 1.119189    Objective Loss 1.119189    Top1 88.508065    Top5 98.588710    LR 0.100000    Time 0.061344    
2024-08-23 16:00:04,830 - --- validate (epoch=0)-----------
2024-08-23 16:00:04,831 - 6000 samples (256 per mini-batch)
2024-08-23 16:00:06,996 - Epoch: [0][   10/   24]    Loss 0.987331    Top1 90.429688    Top5 98.710938    
2024-08-23 16:00:07,111 - Epoch: [0][   20/   24]    Loss 0.991556    Top1 90.058594    Top5 98.652344    
2024-08-23 16:00:07,149 - Epoch: [0][   24/   24]    Loss 0.991510    Top1 90.050000    Top5 98.666667    
2024-08-23 16:00:07,167 - ==> Top1: 90.050    Top5: 98.667    Loss: 0.992

2024-08-23 16:00:07,168 - ==> Confusion:
[[553   1  13   5   0   5  10   5   7   6]
 [  1 667   5   3   5   1   2   4   0   0]
 [  0   2 526  13   5   4   2  25   5   4]
 [  1   0  21 547   0   3   1   6   1   3]
 [  3   4   8   1 500   4   3   8   6  28]
 [  0   2   7  16   2 472   4   0  10   5]
 [  6   2  12   0   5  27 563   0  11   5]
 [  1   4  13  12   3  15   0 568   0   9]
 [  5   0   7   6   4  14  24   1 509  14]
 [ 18   1   4   8  14  13   6  12  11 528]]

2024-08-23 16:00:07,186 - ==> Best [Top1: 90.050   Top5: 98.667   Params: 78072 on epoch: 0]
2024-08-23 16:00:07,186 - Saving checkpoint to: logs/2024.08.23-155951/checkpoint.pth.tar
2024-08-23 16:00:07,195 - 

2024-08-23 16:00:07,196 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 16:00:07,943 - Epoch: [1][   10/  211]    Overall Loss 0.993144    Objective Loss 0.993144                                        LR 0.100000    Time 0.074686    
2024-08-23 16:00:08,083 - Epoch: [1][   20/  211]    Overall Loss 0.991704    Objective Loss 0.991704                                        LR 0.100000    Time 0.044285    
2024-08-23 16:00:08,240 - Epoch: [1][   30/  211]    Overall Loss 0.985653    Objective Loss 0.985653                                        LR 0.100000    Time 0.034751    
2024-08-23 16:00:08,362 - Epoch: [1][   40/  211]    Overall Loss 0.986277    Objective Loss 0.986277                                        LR 0.100000    Time 0.029107    
2024-08-23 16:00:08,526 - Epoch: [1][   50/  211]    Overall Loss 0.987049    Objective Loss 0.987049                                        LR 0.100000    Time 0.026566    
2024-08-23 16:00:08,660 - Epoch: [1][   60/  211]    Overall Loss 0.984424    Objective Loss 0.984424                                        LR 0.100000    Time 0.024359    
2024-08-23 16:00:08,815 - Epoch: [1][   70/  211]    Overall Loss 0.982661    Objective Loss 0.982661                                        LR 0.100000    Time 0.023090    
2024-08-23 16:00:08,955 - Epoch: [1][   80/  211]    Overall Loss 0.980939    Objective Loss 0.980939                                        LR 0.100000    Time 0.021944    
2024-08-23 16:00:09,104 - Epoch: [1][   90/  211]    Overall Loss 0.979658    Objective Loss 0.979658                                        LR 0.100000    Time 0.021161    
2024-08-23 16:00:09,239 - Epoch: [1][  100/  211]    Overall Loss 0.978435    Objective Loss 0.978435                                        LR 0.100000    Time 0.020393    
2024-08-23 16:00:09,385 - Epoch: [1][  110/  211]    Overall Loss 0.977671    Objective Loss 0.977671                                        LR 0.100000    Time 0.019859    
2024-08-23 16:00:09,513 - Epoch: [1][  120/  211]    Overall Loss 0.976384    Objective Loss 0.976384                                        LR 0.100000    Time 0.019263    
2024-08-23 16:00:09,682 - Epoch: [1][  130/  211]    Overall Loss 0.976404    Objective Loss 0.976404                                        LR 0.100000    Time 0.019078    
2024-08-23 16:00:09,815 - Epoch: [1][  140/  211]    Overall Loss 0.975193    Objective Loss 0.975193                                        LR 0.100000    Time 0.018663    
2024-08-23 16:00:09,950 - Epoch: [1][  150/  211]    Overall Loss 0.974224    Objective Loss 0.974224                                        LR 0.100000    Time 0.018320    
2024-08-23 16:00:10,087 - Epoch: [1][  160/  211]    Overall Loss 0.973293    Objective Loss 0.973293                                        LR 0.100000    Time 0.018027    
2024-08-23 16:00:10,220 - Epoch: [1][  170/  211]    Overall Loss 0.972022    Objective Loss 0.972022                                        LR 0.100000    Time 0.017745    
2024-08-23 16:00:10,352 - Epoch: [1][  180/  211]    Overall Loss 0.971147    Objective Loss 0.971147                                        LR 0.100000    Time 0.017493    
2024-08-23 16:00:10,496 - Epoch: [1][  190/  211]    Overall Loss 0.969674    Objective Loss 0.969674                                        LR 0.100000    Time 0.017324    
2024-08-23 16:00:10,618 - Epoch: [1][  200/  211]    Overall Loss 0.969111    Objective Loss 0.969111                                        LR 0.100000    Time 0.017068    
2024-08-23 16:00:10,762 - Epoch: [1][  210/  211]    Overall Loss 0.968678    Objective Loss 0.968678    Top1 92.578125    Top5 98.828125    LR 0.100000    Time 0.016940    
2024-08-23 16:00:10,773 - Epoch: [1][  211/  211]    Overall Loss 0.968679    Objective Loss 0.968679    Top1 91.935484    Top5 98.387097    LR 0.100000    Time 0.016908    
2024-08-23 16:00:10,793 - --- validate (epoch=1)-----------
2024-08-23 16:00:10,793 - 6000 samples (256 per mini-batch)
2024-08-23 16:00:11,125 - Epoch: [1][   10/   24]    Loss 0.948203    Top1 92.617188    Top5 98.515625    
2024-08-23 16:00:11,248 - Epoch: [1][   20/   24]    Loss 0.947880    Top1 92.539062    Top5 98.691406    
2024-08-23 16:00:11,290 - Epoch: [1][   24/   24]    Loss 0.947777    Top1 92.550000    Top5 98.783333    
2024-08-23 16:00:11,307 - ==> Top1: 92.550    Top5: 98.783    Loss: 0.948

2024-08-23 16:00:11,308 - ==> Confusion:
[[586   1   2   2   0   2   4   1   4   3]
 [  4 669   6   0   3   2   0   4   0   0]
 [  3   4 531   2   2   3   0  28   8   5]
 [  1   0  12 554   1   5   1   5   2   2]
 [  6   6   5   0 519   1   2  10   1  15]
 [ 13   3   2   4   1 479   2   7   5   2]
 [ 16   6   5   0   3  22 572   0   7   0]
 [  1   3   8   6   5   4   0 596   0   2]
 [  8   1   4   4   6  12  12   2 519  16]
 [ 16   1   2   3  20   4   1  17   6 545]]

2024-08-23 16:00:11,309 - ==> Best [Top1: 92.550   Top5: 98.783   Params: 78072 on epoch: 1]
2024-08-23 16:00:11,309 - Saving checkpoint to: logs/2024.08.23-155951/checkpoint.pth.tar
2024-08-23 16:00:11,321 - 

2024-08-23 16:00:11,321 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 16:00:11,727 - Epoch: [2][   10/  211]    Overall Loss 0.948719    Objective Loss 0.948719                                        LR 0.100000    Time 0.040459    
2024-08-23 16:00:11,850 - Epoch: [2][   20/  211]    Overall Loss 0.944722    Objective Loss 0.944722                                        LR 0.100000    Time 0.026372    
2024-08-23 16:00:12,007 - Epoch: [2][   30/  211]    Overall Loss 0.946571    Objective Loss 0.946571                                        LR 0.100000    Time 0.022809    
2024-08-23 16:00:12,137 - Epoch: [2][   40/  211]    Overall Loss 0.946731    Objective Loss 0.946731                                        LR 0.100000    Time 0.020344    
2024-08-23 16:00:12,285 - Epoch: [2][   50/  211]    Overall Loss 0.946325    Objective Loss 0.946325                                        LR 0.100000    Time 0.019216    
2024-08-23 16:00:12,408 - Epoch: [2][   60/  211]    Overall Loss 0.947483    Objective Loss 0.947483                                        LR 0.100000    Time 0.018063    
2024-08-23 16:00:12,558 - Epoch: [2][   70/  211]    Overall Loss 0.945701    Objective Loss 0.945701                                        LR 0.100000    Time 0.017620    
2024-08-23 16:00:12,695 - Epoch: [2][   80/  211]    Overall Loss 0.943838    Objective Loss 0.943838                                        LR 0.100000    Time 0.017122    
2024-08-23 16:00:12,842 - Epoch: [2][   90/  211]    Overall Loss 0.944831    Objective Loss 0.944831                                        LR 0.100000    Time 0.016855    
2024-08-23 16:00:12,967 - Epoch: [2][  100/  211]    Overall Loss 0.944069    Objective Loss 0.944069                                        LR 0.100000    Time 0.016418    
2024-08-23 16:00:13,096 - Epoch: [2][  110/  211]    Overall Loss 0.943352    Objective Loss 0.943352                                        LR 0.100000    Time 0.016090    
2024-08-23 16:00:13,233 - Epoch: [2][  120/  211]    Overall Loss 0.943399    Objective Loss 0.943399                                        LR 0.100000    Time 0.015887    
2024-08-23 16:00:13,360 - Epoch: [2][  130/  211]    Overall Loss 0.943887    Objective Loss 0.943887                                        LR 0.100000    Time 0.015639    
2024-08-23 16:00:13,493 - Epoch: [2][  140/  211]    Overall Loss 0.945088    Objective Loss 0.945088                                        LR 0.100000    Time 0.015468    
2024-08-23 16:00:13,628 - Epoch: [2][  150/  211]    Overall Loss 0.944816    Objective Loss 0.944816                                        LR 0.100000    Time 0.015336    
2024-08-23 16:00:13,747 - Epoch: [2][  160/  211]    Overall Loss 0.944460    Objective Loss 0.944460                                        LR 0.100000    Time 0.015120    
2024-08-23 16:00:13,875 - Epoch: [2][  170/  211]    Overall Loss 0.944427    Objective Loss 0.944427                                        LR 0.100000    Time 0.014982    
2024-08-23 16:00:14,004 - Epoch: [2][  180/  211]    Overall Loss 0.944305    Objective Loss 0.944305                                        LR 0.100000    Time 0.014866    
2024-08-23 16:00:14,129 - Epoch: [2][  190/  211]    Overall Loss 0.943133    Objective Loss 0.943133                                        LR 0.100000    Time 0.014736    
2024-08-23 16:00:14,263 - Epoch: [2][  200/  211]    Overall Loss 0.942706    Objective Loss 0.942706                                        LR 0.100000    Time 0.014669    
2024-08-23 16:00:14,394 - Epoch: [2][  210/  211]    Overall Loss 0.941705    Objective Loss 0.941705    Top1 93.359375    Top5 99.218750    LR 0.100000    Time 0.014590    
2024-08-23 16:00:14,404 - Epoch: [2][  211/  211]    Overall Loss 0.941545    Objective Loss 0.941545    Top1 94.354839    Top5 99.193548    LR 0.100000    Time 0.014568    
2024-08-23 16:00:14,421 - --- validate (epoch=2)-----------
2024-08-23 16:00:14,421 - 6000 samples (256 per mini-batch)
2024-08-23 16:00:14,802 - Epoch: [2][   10/   24]    Loss 0.930229    Top1 93.437500    Top5 99.218750    
2024-08-23 16:00:14,919 - Epoch: [2][   20/   24]    Loss 0.932055    Top1 93.222656    Top5 99.140625    
2024-08-23 16:00:14,964 - Epoch: [2][   24/   24]    Loss 0.931592    Top1 93.283333    Top5 99.133333    
2024-08-23 16:00:14,980 - ==> Top1: 93.283    Top5: 99.133    Loss: 0.932

2024-08-23 16:00:14,981 - ==> Confusion:
[[588   0   6   0   3   2   2   1   3   0]
 [  4 658   5   2  10   1   1   7   0   0]
 [  3   0 561   3   1   1   1  11   4   1]
 [  1   0  16 547   1   4   0   3   7   4]
 [  1   0  12   0 533   1   0   1   2  15]
 [  1   0  10   6   5 484   4   2   3   3]
 [  9   0   6   0  15  17 571   0  12   1]
 [  3   0  22   2  14   6   0 572   1   5]
 [  2   0  15   1   8   7   9   2 532   8]
 [ 11   1   6   2  24   4   1   7  14 545]]

2024-08-23 16:00:14,983 - ==> Best [Top1: 93.283   Top5: 99.133   Params: 78072 on epoch: 2]
2024-08-23 16:00:14,983 - Saving checkpoint to: logs/2024.08.23-155951/checkpoint.pth.tar
2024-08-23 16:00:14,990 - --- test ---------------------
2024-08-23 16:00:14,990 - 10000 samples (256 per mini-batch)
2024-08-23 16:00:15,287 - Test: [   10/   40]    Loss 0.844926    Top1 97.773438    Top5 99.882812    
2024-08-23 16:00:15,365 - Test: [   20/   40]    Loss 0.844459    Top1 97.871094    Top5 99.882812    
2024-08-23 16:00:15,437 - Test: [   30/   40]    Loss 0.844963    Top1 97.929688    Top5 99.843750    
2024-08-23 16:00:15,508 - Test: [   40/   40]    Loss 0.844905    Top1 97.800000    Top5 99.830000    
2024-08-23 16:00:15,525 - ==> Top1: 97.800    Top5: 99.830    Loss: 0.845

2024-08-23 16:00:15,526 - ==> Confusion:
[[ 974    0    1    1    0    0    1    0    3    0]
 [   0 1117    4    2    4    0    3    5    0    0]
 [   7    0 1017    3    2    0    0    1    2    0]
 [   0    0   11  994    0    1    0    2    2    0]
 [   0    0    4    0  970    0    1    1    0    6]
 [   2    0    0    6    0  880    2    0    1    1]
 [  12    3    1    0    4    2  934    0    2    0]
 [   1    1   17    2    7    0    0  993    1    6]
 [   6    0    5    5   10    3    7    2  931    5]
 [   5    0    2    2   22    3    0    3    5  967]]

2024-08-23 16:00:15,530 - 
2024-08-23 16:00:15,530 - Log file for this run: /root/Summer_Project/ai8x-training/logs/2024.08.23-155951/2024.08.23-155951.log
