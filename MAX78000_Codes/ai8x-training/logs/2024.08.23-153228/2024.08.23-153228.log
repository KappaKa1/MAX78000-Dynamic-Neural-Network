2024-08-23 15:32:28,717 - Log file for this run: /root/Summer_Project/ai8x-training/logs/2024.08.23-153228/2024.08.23-153228.log
2024-08-23 15:32:28,718 - The open file limit is 1024. Please raise the limit (see documentation).
2024-08-23 15:32:28,718 - Configuring device: MAX78000, simulate=False.
2024-08-23 15:32:28,905 - => loading checkpoint logs/2024.08.23-151148/checkpoint.pth.tar
2024-08-23 15:32:28,909 - => Checkpoint contents:
+----------------------+-------------+----------+
| Key                  | Type        | Value    |
|----------------------+-------------+----------|
| arch                 | str         | mnist_75 |
| compression_sched    | dict        |          |
| epoch                | int         | 4        |
| extras               | dict        |          |
| optimizer_state_dict | dict        |          |
| optimizer_type       | type        | SGD      |
| state_dict           | OrderedDict |          |
+----------------------+-------------+----------+

2024-08-23 15:32:28,910 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  4      |
| best_mAP     | int    |  0      |
| best_top1    | float  | 94.4333 |
| current_mAP  | int    |  0      |
| current_top1 | float  | 94.4333 |
+--------------+--------+---------+

2024-08-23 15:32:28,910 - Loaded compression schedule from checkpoint (epoch 4)
2024-08-23 15:32:28,920 - => loaded 'state_dict' from checkpoint 'logs/2024.08.23-151148/checkpoint.pth.tar'
2024-08-23 15:32:28,950 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-08-23 15:32:28,950 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}
2024-08-23 15:32:29,643 - torch.compile() successful, mode=default, cache limit=8
2024-08-23 15:32:29,643 - Dataset sizes:
	training=54000
	validation=6000
	test=10000
2024-08-23 15:32:29,643 - 

2024-08-23 15:32:29,643 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:32:31,400 - /root/Summer_Project/ai8x-training/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(

2024-08-23 15:32:36,393 - Epoch: [0][   10/  211]    Overall Loss 2.059647    Objective Loss 2.059647                                        LR 0.100000    Time 0.674957    
2024-08-23 15:32:36,504 - Epoch: [0][   20/  211]    Overall Loss 1.776737    Objective Loss 1.776737                                        LR 0.100000    Time 0.342959    
2024-08-23 15:32:36,616 - Epoch: [0][   30/  211]    Overall Loss 1.597327    Objective Loss 1.597327                                        LR 0.100000    Time 0.232361    
2024-08-23 15:32:36,735 - Epoch: [0][   40/  211]    Overall Loss 1.496295    Objective Loss 1.496295                                        LR 0.100000    Time 0.177229    
2024-08-23 15:32:36,855 - Epoch: [0][   50/  211]    Overall Loss 1.418980    Objective Loss 1.418980                                        LR 0.100000    Time 0.144172    
2024-08-23 15:32:36,985 - Epoch: [0][   60/  211]    Overall Loss 1.366910    Objective Loss 1.366910                                        LR 0.100000    Time 0.122308    
2024-08-23 15:32:37,114 - Epoch: [0][   70/  211]    Overall Loss 1.323117    Objective Loss 1.323117                                        LR 0.100000    Time 0.106671    
2024-08-23 15:32:37,234 - Epoch: [0][   80/  211]    Overall Loss 1.285527    Objective Loss 1.285527                                        LR 0.100000    Time 0.094828    
2024-08-23 15:32:37,363 - Epoch: [0][   90/  211]    Overall Loss 1.255869    Objective Loss 1.255869                                        LR 0.100000    Time 0.085723    
2024-08-23 15:32:37,490 - Epoch: [0][  100/  211]    Overall Loss 1.231377    Objective Loss 1.231377                                        LR 0.100000    Time 0.078422    
2024-08-23 15:32:37,613 - Epoch: [0][  110/  211]    Overall Loss 1.209980    Objective Loss 1.209980                                        LR 0.100000    Time 0.072404    
2024-08-23 15:32:37,731 - Epoch: [0][  120/  211]    Overall Loss 1.192987    Objective Loss 1.192987                                        LR 0.100000    Time 0.067354    
2024-08-23 15:32:37,844 - Epoch: [0][  130/  211]    Overall Loss 1.177713    Objective Loss 1.177713                                        LR 0.100000    Time 0.063036    
2024-08-23 15:32:37,965 - Epoch: [0][  140/  211]    Overall Loss 1.165306    Objective Loss 1.165306                                        LR 0.100000    Time 0.059395    
2024-08-23 15:32:38,076 - Epoch: [0][  150/  211]    Overall Loss 1.154423    Objective Loss 1.154423                                        LR 0.100000    Time 0.056174    
2024-08-23 15:32:38,200 - Epoch: [0][  160/  211]    Overall Loss 1.144534    Objective Loss 1.144534                                        LR 0.100000    Time 0.053434    
2024-08-23 15:32:38,315 - Epoch: [0][  170/  211]    Overall Loss 1.136072    Objective Loss 1.136072                                        LR 0.100000    Time 0.050968    
2024-08-23 15:32:38,439 - Epoch: [0][  180/  211]    Overall Loss 1.127011    Objective Loss 1.127011                                        LR 0.100000    Time 0.048822    
2024-08-23 15:32:38,552 - Epoch: [0][  190/  211]    Overall Loss 1.119371    Objective Loss 1.119371                                        LR 0.100000    Time 0.046846    
2024-08-23 15:32:38,668 - Epoch: [0][  200/  211]    Overall Loss 1.112913    Objective Loss 1.112913                                        LR 0.100000    Time 0.045079    
2024-08-23 15:32:38,785 - Epoch: [0][  210/  211]    Overall Loss 1.105809    Objective Loss 1.105809    Top1 93.750000    Top5 99.609375    LR 0.100000    Time 0.043492    
2024-08-23 15:32:43,812 - Epoch: [0][  211/  211]    Overall Loss 1.104955    Objective Loss 1.104955    Top1 94.556452    Top5 99.596774    LR 0.100000    Time 0.067110    
2024-08-23 15:32:43,828 - --- validate (epoch=0)-----------
2024-08-23 15:32:43,828 - 6000 samples (256 per mini-batch)
2024-08-23 15:32:46,068 - Epoch: [0][   10/   24]    Loss 0.963755    Top1 92.226562    Top5 98.671875    
2024-08-23 15:32:46,182 - Epoch: [0][   20/   24]    Loss 0.962665    Top1 92.167969    Top5 98.671875    
2024-08-23 15:32:46,342 - Epoch: [0][   24/   24]    Loss 0.967037    Top1 91.933333    Top5 98.633333    
2024-08-23 15:32:46,362 - ==> Top1: 91.933    Top5: 98.633    Loss: 0.967

2024-08-23 15:32:46,363 - ==> Confusion:
[[597   2   5   2   2   5   3   3  13   8]
 [  1 678   5   1   9   0   0   7   0   2]
 [  8   0 523  17   8   1   3  18  10   9]
 [  0   0   7 576   0   7   0   2   7   2]
 [  4   2   3   2 532   1   3   1   1   8]
 [  5   2   2   3   3 483   3   0   3   6]
 [ 13   2   5   0  17  13 536   0  14   7]
 [  2   0  14  12  16   7   0 547   1   7]
 [  3   0   9   3  10  14   1   0 523  14]
 [  7   2   1   8  20   6   2   4   4 548]]

2024-08-23 15:32:46,399 - ==> Best [Top1: 91.933   Top5: 98.633   Params: 78072 on epoch: 0]
2024-08-23 15:32:46,400 - Saving checkpoint to: logs/2024.08.23-153228/checkpoint.pth.tar
2024-08-23 15:32:46,410 - 

2024-08-23 15:32:46,410 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:32:46,929 - Epoch: [1][   10/  211]    Overall Loss 0.976130    Objective Loss 0.976130                                        LR 0.100000    Time 0.051821    
2024-08-23 15:32:47,080 - Epoch: [1][   20/  211]    Overall Loss 0.976306    Objective Loss 0.976306                                        LR 0.100000    Time 0.033442    
2024-08-23 15:32:47,197 - Epoch: [1][   30/  211]    Overall Loss 0.975193    Objective Loss 0.975193                                        LR 0.100000    Time 0.026187    
2024-08-23 15:32:47,318 - Epoch: [1][   40/  211]    Overall Loss 0.971490    Objective Loss 0.971490                                        LR 0.100000    Time 0.022634    
2024-08-23 15:32:47,533 - Epoch: [1][   50/  211]    Overall Loss 0.969257    Objective Loss 0.969257                                        LR 0.100000    Time 0.022407    
2024-08-23 15:32:47,689 - Epoch: [1][   60/  211]    Overall Loss 0.965436    Objective Loss 0.965436                                        LR 0.100000    Time 0.021269    
2024-08-23 15:32:47,853 - Epoch: [1][   70/  211]    Overall Loss 0.962334    Objective Loss 0.962334                                        LR 0.100000    Time 0.020571    
2024-08-23 15:32:48,003 - Epoch: [1][   80/  211]    Overall Loss 0.962469    Objective Loss 0.962469                                        LR 0.100000    Time 0.019869    
2024-08-23 15:32:48,138 - Epoch: [1][   90/  211]    Overall Loss 0.960892    Objective Loss 0.960892                                        LR 0.100000    Time 0.019149    
2024-08-23 15:32:48,279 - Epoch: [1][  100/  211]    Overall Loss 0.959449    Objective Loss 0.959449                                        LR 0.100000    Time 0.018642    
2024-08-23 15:32:48,430 - Epoch: [1][  110/  211]    Overall Loss 0.959235    Objective Loss 0.959235                                        LR 0.100000    Time 0.018321    
2024-08-23 15:32:48,577 - Epoch: [1][  120/  211]    Overall Loss 0.957953    Objective Loss 0.957953                                        LR 0.100000    Time 0.018015    
2024-08-23 15:32:48,724 - Epoch: [1][  130/  211]    Overall Loss 0.957241    Objective Loss 0.957241                                        LR 0.100000    Time 0.017753    
2024-08-23 15:32:48,853 - Epoch: [1][  140/  211]    Overall Loss 0.957786    Objective Loss 0.957786                                        LR 0.100000    Time 0.017403    
2024-08-23 15:32:48,981 - Epoch: [1][  150/  211]    Overall Loss 0.956468    Objective Loss 0.956468                                        LR 0.100000    Time 0.017094    
2024-08-23 15:32:49,128 - Epoch: [1][  160/  211]    Overall Loss 0.955741    Objective Loss 0.955741                                        LR 0.100000    Time 0.016944    
2024-08-23 15:32:49,266 - Epoch: [1][  170/  211]    Overall Loss 0.955482    Objective Loss 0.955482                                        LR 0.100000    Time 0.016757    
2024-08-23 15:32:49,402 - Epoch: [1][  180/  211]    Overall Loss 0.955448    Objective Loss 0.955448                                        LR 0.100000    Time 0.016577    
2024-08-23 15:32:49,541 - Epoch: [1][  190/  211]    Overall Loss 0.955681    Objective Loss 0.955681                                        LR 0.100000    Time 0.016436    
2024-08-23 15:32:49,696 - Epoch: [1][  200/  211]    Overall Loss 0.955165    Objective Loss 0.955165                                        LR 0.100000    Time 0.016387    
2024-08-23 15:32:49,834 - Epoch: [1][  210/  211]    Overall Loss 0.954154    Objective Loss 0.954154    Top1 89.843750    Top5 99.609375    LR 0.100000    Time 0.016261    
2024-08-23 15:32:49,850 - Epoch: [1][  211/  211]    Overall Loss 0.954046    Objective Loss 0.954046    Top1 90.725806    Top5 99.596774    LR 0.100000    Time 0.016258    
2024-08-23 15:32:49,872 - --- validate (epoch=1)-----------
2024-08-23 15:32:49,872 - 6000 samples (256 per mini-batch)
2024-08-23 15:32:50,242 - Epoch: [1][   10/   24]    Loss 0.950364    Top1 91.796875    Top5 98.789062    
2024-08-23 15:32:50,361 - Epoch: [1][   20/   24]    Loss 0.947762    Top1 91.933594    Top5 98.847656    
2024-08-23 15:32:50,405 - Epoch: [1][   24/   24]    Loss 0.948151    Top1 92.016667    Top5 98.833333    
2024-08-23 15:32:50,424 - ==> Top1: 92.017    Top5: 98.833    Loss: 0.948

2024-08-23 15:32:50,425 - ==> Confusion:
[[605   1   9   2   0   2   8   0   4   9]
 [  0 686   7   2   3   0   0   4   0   1]
 [  5   1 551   8   2   4   4  14   3   5]
 [  0   0  13 572   0  10   0   2   3   1]
 [  6   2  13   1 487   3   2  13   2  28]
 [  2   4   7   7   1 477   4   2   2   4]
 [  5   4  10   0   1  40 543   0   3   1]
 [  1   1  26   7   6   1   0 556   0   8]
 [  2   2   8  11   2  14  10   1 506  21]
 [  1   2   6   7   3   4   3   7   5 564]]

2024-08-23 15:32:50,427 - ==> Best [Top1: 92.017   Top5: 98.833   Params: 78072 on epoch: 1]
2024-08-23 15:32:50,427 - Saving checkpoint to: logs/2024.08.23-153228/checkpoint.pth.tar
2024-08-23 15:32:50,434 - 

2024-08-23 15:32:50,434 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:32:50,911 - Epoch: [2][   10/  211]    Overall Loss 0.946251    Objective Loss 0.946251                                        LR 0.100000    Time 0.047592    
2024-08-23 15:32:51,047 - Epoch: [2][   20/  211]    Overall Loss 0.938962    Objective Loss 0.938962                                        LR 0.100000    Time 0.030571    
2024-08-23 15:32:51,191 - Epoch: [2][   30/  211]    Overall Loss 0.938284    Objective Loss 0.938284                                        LR 0.100000    Time 0.025144    
2024-08-23 15:32:51,326 - Epoch: [2][   40/  211]    Overall Loss 0.937647    Objective Loss 0.937647                                        LR 0.100000    Time 0.022221    
2024-08-23 15:32:51,469 - Epoch: [2][   50/  211]    Overall Loss 0.936351    Objective Loss 0.936351                                        LR 0.100000    Time 0.020634    
2024-08-23 15:32:51,599 - Epoch: [2][   60/  211]    Overall Loss 0.936752    Objective Loss 0.936752                                        LR 0.100000    Time 0.019352    
2024-08-23 15:32:51,736 - Epoch: [2][   70/  211]    Overall Loss 0.936917    Objective Loss 0.936917                                        LR 0.100000    Time 0.018553    
2024-08-23 15:32:51,862 - Epoch: [2][   80/  211]    Overall Loss 0.938440    Objective Loss 0.938440                                        LR 0.100000    Time 0.017794    
2024-08-23 15:32:51,988 - Epoch: [2][   90/  211]    Overall Loss 0.937976    Objective Loss 0.937976                                        LR 0.100000    Time 0.017218    
2024-08-23 15:32:52,107 - Epoch: [2][  100/  211]    Overall Loss 0.936896    Objective Loss 0.936896                                        LR 0.100000    Time 0.016680    
2024-08-23 15:32:52,239 - Epoch: [2][  110/  211]    Overall Loss 0.937848    Objective Loss 0.937848                                        LR 0.100000    Time 0.016366    
2024-08-23 15:32:52,353 - Epoch: [2][  120/  211]    Overall Loss 0.936605    Objective Loss 0.936605                                        LR 0.100000    Time 0.015948    
2024-08-23 15:32:52,486 - Epoch: [2][  130/  211]    Overall Loss 0.935616    Objective Loss 0.935616                                        LR 0.100000    Time 0.015738    
2024-08-23 15:32:52,600 - Epoch: [2][  140/  211]    Overall Loss 0.934763    Objective Loss 0.934763                                        LR 0.100000    Time 0.015427    
2024-08-23 15:32:52,724 - Epoch: [2][  150/  211]    Overall Loss 0.934367    Objective Loss 0.934367                                        LR 0.100000    Time 0.015220    
2024-08-23 15:32:52,843 - Epoch: [2][  160/  211]    Overall Loss 0.934232    Objective Loss 0.934232                                        LR 0.100000    Time 0.015014    
2024-08-23 15:32:52,973 - Epoch: [2][  170/  211]    Overall Loss 0.933676    Objective Loss 0.933676                                        LR 0.100000    Time 0.014894    
2024-08-23 15:32:53,090 - Epoch: [2][  180/  211]    Overall Loss 0.933779    Objective Loss 0.933779                                        LR 0.100000    Time 0.014715    
2024-08-23 15:32:53,222 - Epoch: [2][  190/  211]    Overall Loss 0.933212    Objective Loss 0.933212                                        LR 0.100000    Time 0.014629    
2024-08-23 15:32:53,342 - Epoch: [2][  200/  211]    Overall Loss 0.932722    Objective Loss 0.932722                                        LR 0.100000    Time 0.014496    
2024-08-23 15:32:53,468 - Epoch: [2][  210/  211]    Overall Loss 0.932604    Objective Loss 0.932604    Top1 93.359375    Top5 99.609375    LR 0.100000    Time 0.014407    
2024-08-23 15:32:53,479 - Epoch: [2][  211/  211]    Overall Loss 0.932827    Objective Loss 0.932827    Top1 91.733871    Top5 99.193548    LR 0.100000    Time 0.014390    
2024-08-23 15:32:53,500 - --- validate (epoch=2)-----------
2024-08-23 15:32:53,500 - 6000 samples (256 per mini-batch)
2024-08-23 15:32:53,858 - Epoch: [2][   10/   24]    Loss 0.923871    Top1 94.296875    Top5 98.906250    
2024-08-23 15:32:53,974 - Epoch: [2][   20/   24]    Loss 0.926604    Top1 94.062500    Top5 98.964844    
2024-08-23 15:32:54,018 - Epoch: [2][   24/   24]    Loss 0.929162    Top1 93.900000    Top5 99.000000    
2024-08-23 15:32:54,038 - ==> Top1: 93.900    Top5: 99.000    Loss: 0.929

2024-08-23 15:32:54,038 - ==> Confusion:
[[618   1   4   0   1   0   8   2   4   2]
 [  0 689   7   1   4   0   0   2   0   0]
 [  3   3 548   1  10   5   4  14   4   5]
 [  4   3  20 547   3   9   0  10   4   1]
 [  2   2   3   0 546   0   1   1   1   1]
 [  6   4   5   2   9 459  15   0   3   7]
 [  9   4   3   0   7   5 575   0   3   1]
 [  1   6  18   0   8   2   0 568   1   2]
 [  7   3   3   1  15   5  11   2 524   6]
 [  4   3   3   1  29   4   1   1   3 553]]

2024-08-23 15:32:54,041 - ==> Best [Top1: 93.900   Top5: 99.000   Params: 78072 on epoch: 2]
2024-08-23 15:32:54,042 - Saving checkpoint to: logs/2024.08.23-153228/checkpoint.pth.tar
2024-08-23 15:32:54,050 - --- test ---------------------
2024-08-23 15:32:54,050 - 10000 samples (256 per mini-batch)
2024-08-23 15:32:54,332 - Test: [   10/   40]    Loss 0.861226    Top1 97.031250    Top5 99.648438    
2024-08-23 15:32:54,407 - Test: [   20/   40]    Loss 0.861854    Top1 96.992188    Top5 99.628906    
2024-08-23 15:32:54,477 - Test: [   30/   40]    Loss 0.862418    Top1 97.005208    Top5 99.557292    
2024-08-23 15:32:54,617 - Test: [   40/   40]    Loss 0.863913    Top1 97.020000    Top5 99.600000    
2024-08-23 15:32:54,638 - ==> Top1: 97.020    Top5: 99.600    Loss: 0.864

2024-08-23 15:32:54,638 - ==> Confusion:
[[ 968    0    2    0    2    0    6    0    1    1]
 [   1 1129    3    1    1    0    0    0    0    0]
 [   6    1 1015    0    1    1    0    6    1    1]
 [   2    2   11  976    1    5    0   10    2    1]
 [   0    3    1    0  968    0    1    0    0    9]
 [   3    3    0    4    2  872    3    0    2    3]
 [  16    7    1    0    6    0  928    0    0    0]
 [   2    9   20    0    9    0    0  987    0    1]
 [  24    0    5    0   34   10   16    1  880    4]
 [   6    5    0    3   27    7    0    7    1  953]]

2024-08-23 15:32:54,643 - 
2024-08-23 15:32:54,643 - Log file for this run: /root/Summer_Project/ai8x-training/logs/2024.08.23-153228/2024.08.23-153228.log
