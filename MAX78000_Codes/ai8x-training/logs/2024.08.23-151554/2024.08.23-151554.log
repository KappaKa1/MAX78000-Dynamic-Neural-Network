2024-08-23 15:15:54,232 - Log file for this run: /root/Summer_Project/ai8x-training/logs/2024.08.23-151554/2024.08.23-151554.log
2024-08-23 15:15:54,232 - The open file limit is 1024. Please raise the limit (see documentation).
2024-08-23 15:15:54,233 - Configuring device: MAX78000, simulate=False.
2024-08-23 15:15:54,442 - => loading checkpoint logs/2024.08.23-151148/checkpoint.pth.tar
2024-08-23 15:15:54,446 - => Checkpoint contents:
+----------------------+-------------+----------+
| Key                  | Type        | Value    |
|----------------------+-------------+----------|
| arch                 | str         | mnist_75 |
| compression_sched    | dict        |          |
| epoch                | int         | 4        |
| extras               | dict        |          |
| optimizer_state_dict | dict        |          |
| optimizer_type       | type        | SGD      |
| state_dict           | OrderedDict |          |
+----------------------+-------------+----------+

2024-08-23 15:15:54,447 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  4      |
| best_mAP     | int    |  0      |
| best_top1    | float  | 94.4333 |
| current_mAP  | int    |  0      |
| current_top1 | float  | 94.4333 |
+--------------+--------+---------+

2024-08-23 15:15:54,447 - Loaded compression schedule from checkpoint (epoch 4)
2024-08-23 15:15:54,459 - => loaded 'state_dict' from checkpoint 'logs/2024.08.23-151148/checkpoint.pth.tar'
2024-08-23 15:15:54,494 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-08-23 15:15:54,494 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}
2024-08-23 15:15:55,050 - torch.compile() successful, mode=default, cache limit=8
2024-08-23 15:15:55,050 - Dataset sizes:
	training=54000
	validation=6000
	test=10000
2024-08-23 15:15:55,050 - 

2024-08-23 15:15:55,051 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:15:56,659 - /root/Summer_Project/ai8x-training/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(

2024-08-23 15:16:01,329 - Epoch: [0][   10/  211]    Overall Loss 2.234546    Objective Loss 2.234546                                        LR 0.100000    Time 0.627742    
2024-08-23 15:16:01,449 - Epoch: [0][   20/  211]    Overall Loss 2.029965    Objective Loss 2.029965                                        LR 0.100000    Time 0.319887    
2024-08-23 15:16:01,609 - Epoch: [0][   30/  211]    Overall Loss 1.931747    Objective Loss 1.931747                                        LR 0.100000    Time 0.218560    
2024-08-23 15:16:01,762 - Epoch: [0][   40/  211]    Overall Loss 1.844282    Objective Loss 1.844282                                        LR 0.100000    Time 0.167755    
2024-08-23 15:16:01,940 - Epoch: [0][   50/  211]    Overall Loss 1.774508    Objective Loss 1.774508                                        LR 0.100000    Time 0.137744    
2024-08-23 15:16:02,072 - Epoch: [0][   60/  211]    Overall Loss 1.715900    Objective Loss 1.715900                                        LR 0.100000    Time 0.116980    
2024-08-23 15:16:02,222 - Epoch: [0][   70/  211]    Overall Loss 1.668912    Objective Loss 1.668912                                        LR 0.100000    Time 0.102404    
2024-08-23 15:16:02,360 - Epoch: [0][   80/  211]    Overall Loss 1.628728    Objective Loss 1.628728                                        LR 0.100000    Time 0.091327    
2024-08-23 15:16:02,505 - Epoch: [0][   90/  211]    Overall Loss 1.592203    Objective Loss 1.592203                                        LR 0.100000    Time 0.082791    
2024-08-23 15:16:02,643 - Epoch: [0][  100/  211]    Overall Loss 1.563095    Objective Loss 1.563095                                        LR 0.100000    Time 0.075888    
2024-08-23 15:16:02,804 - Epoch: [0][  110/  211]    Overall Loss 1.539526    Objective Loss 1.539526                                        LR 0.100000    Time 0.070447    
2024-08-23 15:16:02,940 - Epoch: [0][  120/  211]    Overall Loss 1.518561    Objective Loss 1.518561                                        LR 0.100000    Time 0.065702    
2024-08-23 15:16:03,073 - Epoch: [0][  130/  211]    Overall Loss 1.498518    Objective Loss 1.498518                                        LR 0.100000    Time 0.061669    
2024-08-23 15:16:03,203 - Epoch: [0][  140/  211]    Overall Loss 1.481236    Objective Loss 1.481236                                        LR 0.100000    Time 0.058194    
2024-08-23 15:16:03,337 - Epoch: [0][  150/  211]    Overall Loss 1.465431    Objective Loss 1.465431                                        LR 0.100000    Time 0.055201    
2024-08-23 15:16:03,475 - Epoch: [0][  160/  211]    Overall Loss 1.449505    Objective Loss 1.449505                                        LR 0.100000    Time 0.052616    
2024-08-23 15:16:03,619 - Epoch: [0][  170/  211]    Overall Loss 1.436672    Objective Loss 1.436672                                        LR 0.100000    Time 0.050362    
2024-08-23 15:16:03,749 - Epoch: [0][  180/  211]    Overall Loss 1.424083    Objective Loss 1.424083                                        LR 0.100000    Time 0.048286    
2024-08-23 15:16:03,896 - Epoch: [0][  190/  211]    Overall Loss 1.412617    Objective Loss 1.412617                                        LR 0.100000    Time 0.046517    
2024-08-23 15:16:04,042 - Epoch: [0][  200/  211]    Overall Loss 1.401780    Objective Loss 1.401780                                        LR 0.100000    Time 0.044918    
2024-08-23 15:16:04,175 - Epoch: [0][  210/  211]    Overall Loss 1.392064    Objective Loss 1.392064    Top1 80.078125    Top5 96.875000    LR 0.100000    Time 0.043412    
2024-08-23 15:16:09,394 - Epoch: [0][  211/  211]    Overall Loss 1.390984    Objective Loss 1.390984    Top1 80.040323    Top5 97.379032    LR 0.100000    Time 0.067940    
2024-08-23 15:16:09,414 - --- validate (epoch=0)-----------
2024-08-23 15:16:09,414 - 6000 samples (256 per mini-batch)
2024-08-23 15:16:11,680 - Epoch: [0][   10/   24]    Loss 1.185057    Top1 78.125000    Top5 97.617188    
2024-08-23 15:16:11,803 - Epoch: [0][   20/   24]    Loss 1.192206    Top1 78.222656    Top5 97.441406    
2024-08-23 15:16:11,965 - Epoch: [0][   24/   24]    Loss 1.192644    Top1 78.250000    Top5 97.366667    
2024-08-23 15:16:11,983 - ==> Top1: 78.250    Top5: 97.367    Loss: 1.193

2024-08-23 15:16:11,984 - ==> Confusion:
[[517   1  12   1   1   3  27   5   0   2]
 [  1 639   7   2   2   1   4   5   0   1]
 [ 16   1 531  14   2  10   2  30   0   5]
 [  3   0 101 462   0  22   0  21   0  14]
 [ 10   8  11   0 514  15  20   6   0  26]
 [  3   3  21   4   4 491  11   0   0   8]
 [ 26   4  11   0   7  25 526   0   0   2]
 [  1  10  24  23   3  10   0 544   0  28]
 [122   9  92   1   7 153 110  11   0  66]
 [ 28   1  10   3   5  31   8  33   0 446]]

2024-08-23 15:16:12,001 - ==> Best [Top1: 78.250   Top5: 97.367   Params: 78072 on epoch: 0]
2024-08-23 15:16:12,002 - Saving checkpoint to: logs/2024.08.23-151554/checkpoint.pth.tar
2024-08-23 15:16:12,013 - 

2024-08-23 15:16:12,013 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:16:12,483 - Epoch: [1][   10/  211]    Overall Loss 1.198954    Objective Loss 1.198954                                        LR 0.100000    Time 0.046946    
2024-08-23 15:16:12,603 - Epoch: [1][   20/  211]    Overall Loss 1.200567    Objective Loss 1.200567                                        LR 0.100000    Time 0.029449    
2024-08-23 15:16:12,743 - Epoch: [1][   30/  211]    Overall Loss 1.191992    Objective Loss 1.191992                                        LR 0.100000    Time 0.024283    
2024-08-23 15:16:12,902 - Epoch: [1][   40/  211]    Overall Loss 1.190084    Objective Loss 1.190084                                        LR 0.100000    Time 0.022171    
2024-08-23 15:16:13,083 - Epoch: [1][   50/  211]    Overall Loss 1.189439    Objective Loss 1.189439                                        LR 0.100000    Time 0.021357    
2024-08-23 15:16:13,214 - Epoch: [1][   60/  211]    Overall Loss 1.186518    Objective Loss 1.186518                                        LR 0.100000    Time 0.019976    
2024-08-23 15:16:13,358 - Epoch: [1][   70/  211]    Overall Loss 1.185028    Objective Loss 1.185028                                        LR 0.100000    Time 0.019162    
2024-08-23 15:16:13,499 - Epoch: [1][   80/  211]    Overall Loss 1.183900    Objective Loss 1.183900                                        LR 0.100000    Time 0.018525    
2024-08-23 15:16:13,636 - Epoch: [1][   90/  211]    Overall Loss 1.181999    Objective Loss 1.181999                                        LR 0.100000    Time 0.017984    
2024-08-23 15:16:13,767 - Epoch: [1][  100/  211]    Overall Loss 1.179493    Objective Loss 1.179493                                        LR 0.100000    Time 0.017500    
2024-08-23 15:16:13,904 - Epoch: [1][  110/  211]    Overall Loss 1.179301    Objective Loss 1.179301                                        LR 0.100000    Time 0.017144    
2024-08-23 15:16:14,043 - Epoch: [1][  120/  211]    Overall Loss 1.179320    Objective Loss 1.179320                                        LR 0.100000    Time 0.016870    
2024-08-23 15:16:14,164 - Epoch: [1][  130/  211]    Overall Loss 1.178131    Objective Loss 1.178131                                        LR 0.100000    Time 0.016504    
2024-08-23 15:16:14,292 - Epoch: [1][  140/  211]    Overall Loss 1.177321    Objective Loss 1.177321                                        LR 0.100000    Time 0.016234    
2024-08-23 15:16:14,432 - Epoch: [1][  150/  211]    Overall Loss 1.175059    Objective Loss 1.175059                                        LR 0.100000    Time 0.016084    
2024-08-23 15:16:14,564 - Epoch: [1][  160/  211]    Overall Loss 1.172576    Objective Loss 1.172576                                        LR 0.100000    Time 0.015902    
2024-08-23 15:16:14,695 - Epoch: [1][  170/  211]    Overall Loss 1.171683    Objective Loss 1.171683                                        LR 0.100000    Time 0.015736    
2024-08-23 15:16:14,818 - Epoch: [1][  180/  211]    Overall Loss 1.170211    Objective Loss 1.170211                                        LR 0.100000    Time 0.015541    
2024-08-23 15:16:15,001 - Epoch: [1][  190/  211]    Overall Loss 1.168304    Objective Loss 1.168304                                        LR 0.100000    Time 0.015687    
2024-08-23 15:16:15,132 - Epoch: [1][  200/  211]    Overall Loss 1.167808    Objective Loss 1.167808                                        LR 0.100000    Time 0.015556    
2024-08-23 15:16:15,255 - Epoch: [1][  210/  211]    Overall Loss 1.166633    Objective Loss 1.166633    Top1 79.296875    Top5 98.046875    LR 0.100000    Time 0.015399    
2024-08-23 15:16:15,268 - Epoch: [1][  211/  211]    Overall Loss 1.166411    Objective Loss 1.166411    Top1 81.854839    Top5 97.177419    LR 0.100000    Time 0.015383    
2024-08-23 15:16:15,287 - --- validate (epoch=1)-----------
2024-08-23 15:16:15,287 - 6000 samples (256 per mini-batch)
2024-08-23 15:16:15,634 - Epoch: [1][   10/   24]    Loss 1.135913    Top1 83.515625    Top5 98.085938    
2024-08-23 15:16:15,745 - Epoch: [1][   20/   24]    Loss 1.138118    Top1 83.320312    Top5 98.183594    
2024-08-23 15:16:15,787 - Epoch: [1][   24/   24]    Loss 1.134203    Top1 83.533333    Top5 98.233333    
2024-08-23 15:16:15,805 - ==> Top1: 83.533    Top5: 98.233    Loss: 1.134

2024-08-23 15:16:15,805 - ==> Confusion:
[[539   1   7   3   2   4   8   2   0   3]
 [  0 637   8   0   7   0   0  10   0   0]
 [ 17   4 537  17   3   3   2  19   0   9]
 [  3   0  49 546   1   6   0   9   0   9]
 [ 10   5   8   0 573   2   1   2   0   9]
 [  3   3  10  10   9 497   5   2   0   6]
 [ 36   8   6   0   7  13 521   0   0  10]
 [  6   3  21  22  20   3   0 548   0  20]
 [248   7  61  20  22 101  27   8   0  77]
 [ 21   1   6   7  18  14   1  31   0 466]]

2024-08-23 15:16:15,808 - ==> Best [Top1: 83.533   Top5: 98.233   Params: 78072 on epoch: 1]
2024-08-23 15:16:15,808 - Saving checkpoint to: logs/2024.08.23-151554/checkpoint.pth.tar
2024-08-23 15:16:15,816 - 

2024-08-23 15:16:15,816 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:16:16,262 - Epoch: [2][   10/  211]    Overall Loss 1.125342    Objective Loss 1.125342                                        LR 0.100000    Time 0.044515    
2024-08-23 15:16:16,419 - Epoch: [2][   20/  211]    Overall Loss 1.131393    Objective Loss 1.131393                                        LR 0.100000    Time 0.030096    
2024-08-23 15:16:16,592 - Epoch: [2][   30/  211]    Overall Loss 1.136086    Objective Loss 1.136086                                        LR 0.100000    Time 0.025808    
2024-08-23 15:16:16,733 - Epoch: [2][   40/  211]    Overall Loss 1.136669    Objective Loss 1.136669                                        LR 0.100000    Time 0.022867    
2024-08-23 15:16:16,872 - Epoch: [2][   50/  211]    Overall Loss 1.135668    Objective Loss 1.135668                                        LR 0.100000    Time 0.021071    
2024-08-23 15:16:17,020 - Epoch: [2][   60/  211]    Overall Loss 1.131927    Objective Loss 1.131927                                        LR 0.100000    Time 0.020024    
2024-08-23 15:16:17,235 - Epoch: [2][   70/  211]    Overall Loss 1.130682    Objective Loss 1.130682                                        LR 0.100000    Time 0.020219    
2024-08-23 15:16:17,377 - Epoch: [2][   80/  211]    Overall Loss 1.133068    Objective Loss 1.133068                                        LR 0.100000    Time 0.019461    
2024-08-23 15:16:17,510 - Epoch: [2][   90/  211]    Overall Loss 1.132588    Objective Loss 1.132588                                        LR 0.100000    Time 0.018775    
2024-08-23 15:16:17,635 - Epoch: [2][  100/  211]    Overall Loss 1.133080    Objective Loss 1.133080                                        LR 0.100000    Time 0.018151    
2024-08-23 15:16:17,799 - Epoch: [2][  110/  211]    Overall Loss 1.132404    Objective Loss 1.132404                                        LR 0.100000    Time 0.017988    
2024-08-23 15:16:17,931 - Epoch: [2][  120/  211]    Overall Loss 1.131816    Objective Loss 1.131816                                        LR 0.100000    Time 0.017579    
2024-08-23 15:16:18,121 - Epoch: [2][  130/  211]    Overall Loss 1.130465    Objective Loss 1.130465                                        LR 0.100000    Time 0.017687    
2024-08-23 15:16:18,244 - Epoch: [2][  140/  211]    Overall Loss 1.129512    Objective Loss 1.129512                                        LR 0.100000    Time 0.017298    
2024-08-23 15:16:18,370 - Epoch: [2][  150/  211]    Overall Loss 1.127941    Objective Loss 1.127941                                        LR 0.100000    Time 0.016986    
2024-08-23 15:16:18,522 - Epoch: [2][  160/  211]    Overall Loss 1.126854    Objective Loss 1.126854                                        LR 0.100000    Time 0.016869    
2024-08-23 15:16:18,666 - Epoch: [2][  170/  211]    Overall Loss 1.125941    Objective Loss 1.125941                                        LR 0.100000    Time 0.016723    
2024-08-23 15:16:18,782 - Epoch: [2][  180/  211]    Overall Loss 1.125806    Objective Loss 1.125806                                        LR 0.100000    Time 0.016437    
2024-08-23 15:16:18,919 - Epoch: [2][  190/  211]    Overall Loss 1.125873    Objective Loss 1.125873                                        LR 0.100000    Time 0.016292    
2024-08-23 15:16:19,046 - Epoch: [2][  200/  211]    Overall Loss 1.125901    Objective Loss 1.125901                                        LR 0.100000    Time 0.016110    
2024-08-23 15:16:19,220 - Epoch: [2][  210/  211]    Overall Loss 1.125487    Objective Loss 1.125487    Top1 87.500000    Top5 98.046875    LR 0.100000    Time 0.016167    
2024-08-23 15:16:19,234 - Epoch: [2][  211/  211]    Overall Loss 1.125533    Objective Loss 1.125533    Top1 86.290323    Top5 97.379032    LR 0.100000    Time 0.016159    
2024-08-23 15:16:19,253 - --- validate (epoch=2)-----------
2024-08-23 15:16:19,253 - 6000 samples (256 per mini-batch)
2024-08-23 15:16:19,593 - Epoch: [2][   10/   24]    Loss 1.107955    Top1 86.796875    Top5 97.773438    
2024-08-23 15:16:19,710 - Epoch: [2][   20/   24]    Loss 1.110511    Top1 86.699219    Top5 97.968750    
2024-08-23 15:16:19,758 - Epoch: [2][   24/   24]    Loss 1.109380    Top1 86.650000    Top5 98.050000    
2024-08-23 15:16:19,776 - ==> Top1: 86.650    Top5: 98.050    Loss: 1.109

2024-08-23 15:16:19,776 - ==> Confusion:
[[552   2   1   2   1   1   5   4   0   1]
 [  0 637   5   2  13   0   2   3   0   0]
 [  7   1 543  17  11   3   3  20   0   6]
 [  4   0  21 579   0   2   0  14   0   3]
 [  4   1  11   0 570   3   5   5   0  11]
 [  5   3   7  19   0 493   5   8   0   5]
 [ 22   3   4   1   4  15 543   0   0   9]
 [  0   1  19  18  11   0   0 585   0   9]
 [304   4  48   9  21  66  51  11   0  57]
 [ 13   1   2   6  23   8   1  36   0 475]]

2024-08-23 15:16:19,778 - ==> Best [Top1: 86.650   Top5: 98.050   Params: 78072 on epoch: 2]
2024-08-23 15:16:19,778 - Saving checkpoint to: logs/2024.08.23-151554/checkpoint.pth.tar
2024-08-23 15:16:19,784 - 

2024-08-23 15:16:19,784 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:16:20,219 - Epoch: [3][   10/  211]    Overall Loss 1.123415    Objective Loss 1.123415                                        LR 0.100000    Time 0.043427    
2024-08-23 15:16:20,357 - Epoch: [3][   20/  211]    Overall Loss 1.121383    Objective Loss 1.121383                                        LR 0.100000    Time 0.028584    
2024-08-23 15:16:20,512 - Epoch: [3][   30/  211]    Overall Loss 1.115854    Objective Loss 1.115854                                        LR 0.100000    Time 0.024199    
2024-08-23 15:16:20,650 - Epoch: [3][   40/  211]    Overall Loss 1.112208    Objective Loss 1.112208                                        LR 0.100000    Time 0.021605    
2024-08-23 15:16:20,794 - Epoch: [3][   50/  211]    Overall Loss 1.114811    Objective Loss 1.114811                                        LR 0.100000    Time 0.020145    
2024-08-23 15:16:20,910 - Epoch: [3][   60/  211]    Overall Loss 1.111770    Objective Loss 1.111770                                        LR 0.100000    Time 0.018719    
2024-08-23 15:16:21,038 - Epoch: [3][   70/  211]    Overall Loss 1.107962    Objective Loss 1.107962                                        LR 0.100000    Time 0.017869    
2024-08-23 15:16:21,156 - Epoch: [3][   80/  211]    Overall Loss 1.106966    Objective Loss 1.106966                                        LR 0.100000    Time 0.017107    
2024-08-23 15:16:21,284 - Epoch: [3][   90/  211]    Overall Loss 1.105893    Objective Loss 1.105893                                        LR 0.100000    Time 0.016627    
2024-08-23 15:16:21,408 - Epoch: [3][  100/  211]    Overall Loss 1.106531    Objective Loss 1.106531                                        LR 0.100000    Time 0.016193    
2024-08-23 15:16:21,537 - Epoch: [3][  110/  211]    Overall Loss 1.107904    Objective Loss 1.107904                                        LR 0.100000    Time 0.015892    
2024-08-23 15:16:21,691 - Epoch: [3][  120/  211]    Overall Loss 1.108106    Objective Loss 1.108106                                        LR 0.100000    Time 0.015848    
2024-08-23 15:16:21,845 - Epoch: [3][  130/  211]    Overall Loss 1.106926    Objective Loss 1.106926                                        LR 0.100000    Time 0.015809    
2024-08-23 15:16:21,994 - Epoch: [3][  140/  211]    Overall Loss 1.107238    Objective Loss 1.107238                                        LR 0.100000    Time 0.015743    
2024-08-23 15:16:22,201 - Epoch: [3][  150/  211]    Overall Loss 1.108033    Objective Loss 1.108033                                        LR 0.100000    Time 0.016073    
2024-08-23 15:16:22,407 - Epoch: [3][  160/  211]    Overall Loss 1.107933    Objective Loss 1.107933                                        LR 0.100000    Time 0.016350    
2024-08-23 15:16:22,584 - Epoch: [3][  170/  211]    Overall Loss 1.107515    Objective Loss 1.107515                                        LR 0.100000    Time 0.016426    
2024-08-23 15:16:22,724 - Epoch: [3][  180/  211]    Overall Loss 1.105969    Objective Loss 1.105969                                        LR 0.100000    Time 0.016285    
2024-08-23 15:16:22,870 - Epoch: [3][  190/  211]    Overall Loss 1.105428    Objective Loss 1.105428                                        LR 0.100000    Time 0.016196    
2024-08-23 15:16:22,999 - Epoch: [3][  200/  211]    Overall Loss 1.104743    Objective Loss 1.104743                                        LR 0.100000    Time 0.016031    
2024-08-23 15:16:23,141 - Epoch: [3][  210/  211]    Overall Loss 1.104634    Objective Loss 1.104634    Top1 85.937500    Top5 98.046875    LR 0.100000    Time 0.015939    
2024-08-23 15:16:23,152 - Epoch: [3][  211/  211]    Overall Loss 1.104551    Objective Loss 1.104551    Top1 86.895161    Top5 98.387097    LR 0.100000    Time 0.015917    
2024-08-23 15:16:23,169 - --- validate (epoch=3)-----------
2024-08-23 15:16:23,169 - 6000 samples (256 per mini-batch)
2024-08-23 15:16:23,547 - Epoch: [3][   10/   24]    Loss 1.087290    Top1 86.289062    Top5 98.476562    
2024-08-23 15:16:23,662 - Epoch: [3][   20/   24]    Loss 1.086535    Top1 86.425781    Top5 98.554688    
2024-08-23 15:16:23,698 - Epoch: [3][   24/   24]    Loss 1.088490    Top1 86.400000    Top5 98.533333    
2024-08-23 15:16:23,714 - ==> Top1: 86.400    Top5: 98.533    Loss: 1.088

2024-08-23 15:16:23,715 - ==> Confusion:
[[549   0   7   5   0   2   3   1   0   2]
 [  0 639   7   3   4   2   1   3   0   3]
 [  3   1 573  12   5   5   4   5   0   3]
 [  2   0  28 574   0   9   0   5   0   5]
 [  3   2  19   0 553   4   1   6   0  22]
 [  3   2   3  13   1 514   4   1   0   4]
 [ 19   5   4   0   1  21 548   0   0   3]
 [  0   1  33  35   3   1   0 546   0  24]
 [268   2  70  30   7  68  48   4   0  74]
 [ 11   0   5  11   2  13   0  20   0 503]]

2024-08-23 15:16:23,718 - ==> Best [Top1: 86.650   Top5: 98.050   Params: 78072 on epoch: 2]
2024-08-23 15:16:23,718 - Saving checkpoint to: logs/2024.08.23-151554/checkpoint.pth.tar
2024-08-23 15:16:23,725 - 

2024-08-23 15:16:23,725 - Training epoch: 54000 samples (256 per mini-batch, world size: 1)
2024-08-23 15:16:24,506 - Epoch: [4][   10/  211]    Overall Loss 1.100557    Objective Loss 1.100557                                        LR 0.100000    Time 0.078014    
2024-08-23 15:16:24,627 - Epoch: [4][   20/  211]    Overall Loss 1.098838    Objective Loss 1.098838                                        LR 0.100000    Time 0.045028    
2024-08-23 15:16:24,756 - Epoch: [4][   30/  211]    Overall Loss 1.104195    Objective Loss 1.104195                                        LR 0.100000    Time 0.034322    
2024-08-23 15:16:24,890 - Epoch: [4][   40/  211]    Overall Loss 1.098403    Objective Loss 1.098403                                        LR 0.100000    Time 0.029068    
2024-08-23 15:16:25,023 - Epoch: [4][   50/  211]    Overall Loss 1.098318    Objective Loss 1.098318                                        LR 0.100000    Time 0.025925    
2024-08-23 15:16:25,139 - Epoch: [4][   60/  211]    Overall Loss 1.097627    Objective Loss 1.097627                                        LR 0.100000    Time 0.023530    
2024-08-23 15:16:25,277 - Epoch: [4][   70/  211]    Overall Loss 1.098412    Objective Loss 1.098412                                        LR 0.100000    Time 0.022129    
2024-08-23 15:16:25,412 - Epoch: [4][   80/  211]    Overall Loss 1.098110    Objective Loss 1.098110                                        LR 0.100000    Time 0.021052    
2024-08-23 15:16:25,562 - Epoch: [4][   90/  211]    Overall Loss 1.098098    Objective Loss 1.098098                                        LR 0.100000    Time 0.020369    
2024-08-23 15:16:25,708 - Epoch: [4][  100/  211]    Overall Loss 1.097753    Objective Loss 1.097753                                        LR 0.100000    Time 0.019788    
2024-08-23 15:16:25,859 - Epoch: [4][  110/  211]    Overall Loss 1.097344    Objective Loss 1.097344                                        LR 0.100000    Time 0.019361    
2024-08-23 15:16:25,978 - Epoch: [4][  120/  211]    Overall Loss 1.095955    Objective Loss 1.095955                                        LR 0.100000    Time 0.018737    
2024-08-23 15:16:26,109 - Epoch: [4][  130/  211]    Overall Loss 1.094787    Objective Loss 1.094787                                        LR 0.100000    Time 0.018302    
2024-08-23 15:16:26,223 - Epoch: [4][  140/  211]    Overall Loss 1.093683    Objective Loss 1.093683                                        LR 0.100000    Time 0.017804    
2024-08-23 15:16:26,344 - Epoch: [4][  150/  211]    Overall Loss 1.093083    Objective Loss 1.093083                                        LR 0.100000    Time 0.017420    
2024-08-23 15:16:26,459 - Epoch: [4][  160/  211]    Overall Loss 1.092799    Objective Loss 1.092799                                        LR 0.100000    Time 0.017051    
2024-08-23 15:16:26,612 - Epoch: [4][  170/  211]    Overall Loss 1.091559    Objective Loss 1.091559                                        LR 0.100000    Time 0.016944    
2024-08-23 15:16:26,810 - Epoch: [4][  180/  211]    Overall Loss 1.089656    Objective Loss 1.089656                                        LR 0.100000    Time 0.017100    
2024-08-23 15:16:26,995 - Epoch: [4][  190/  211]    Overall Loss 1.089309    Objective Loss 1.089309                                        LR 0.100000    Time 0.017171    
2024-08-23 15:16:27,159 - Epoch: [4][  200/  211]    Overall Loss 1.088456    Objective Loss 1.088456                                        LR 0.100000    Time 0.017131    
2024-08-23 15:16:27,289 - Epoch: [4][  210/  211]    Overall Loss 1.088594    Objective Loss 1.088594    Top1 85.937500    Top5 99.609375    LR 0.100000    Time 0.016932    
2024-08-23 15:16:27,300 - Epoch: [4][  211/  211]    Overall Loss 1.088567    Objective Loss 1.088567    Top1 86.693548    Top5 99.193548    LR 0.100000    Time 0.016902    
2024-08-23 15:16:27,319 - --- validate (epoch=4)-----------
2024-08-23 15:16:27,320 - 6000 samples (256 per mini-batch)
2024-08-23 15:16:27,703 - Epoch: [4][   10/   24]    Loss 1.065731    Top1 87.734375    Top5 99.023438    
2024-08-23 15:16:27,817 - Epoch: [4][   20/   24]    Loss 1.078389    Top1 87.070312    Top5 98.867188    
2024-08-23 15:16:27,856 - Epoch: [4][   24/   24]    Loss 1.079739    Top1 87.016667    Top5 98.883333    
2024-08-23 15:16:27,873 - ==> Top1: 87.017    Top5: 98.883    Loss: 1.080

2024-08-23 15:16:27,873 - ==> Confusion:
[[539   3   3   4   0   7   6   3   0   4]
 [  0 643   6   1   5   0   1   5   0   1]
 [  3   1 576  10   5   3   2   8   0   3]
 [  2   1  23 577   0   5   0   8   0   7]
 [  4   1  14   0 562   3   4   5   0  17]
 [  4   2  12  11   1 501   8   1   0   5]
 [  3   1   3   0   3  22 567   0   0   2]
 [  0   1  25  15   7   4   0 580   0  11]
 [214   6  68  24   9 121  58   5   0  66]
 [  7   0   3   9   3  11   1  20   0 511]]

2024-08-23 15:16:27,875 - ==> Best [Top1: 87.017   Top5: 98.883   Params: 78072 on epoch: 4]
2024-08-23 15:16:27,875 - Saving checkpoint to: logs/2024.08.23-151554/checkpoint.pth.tar
2024-08-23 15:16:27,882 - --- test ---------------------
2024-08-23 15:16:27,882 - 10000 samples (256 per mini-batch)
2024-08-23 15:16:28,176 - Test: [   10/   40]    Loss 0.997210    Top1 92.734375    Top5 99.570312    
2024-08-23 15:16:28,255 - Test: [   20/   40]    Loss 1.000527    Top1 92.851562    Top5 99.531250    
2024-08-23 15:16:28,343 - Test: [   30/   40]    Loss 1.002671    Top1 92.916667    Top5 99.570312    
2024-08-23 15:16:28,474 - Test: [   40/   40]    Loss 0.995909    Top1 92.930000    Top5 99.590000    
2024-08-23 15:16:28,492 - ==> Top1: 92.930    Top5: 99.590    Loss: 0.996

2024-08-23 15:16:28,493 - ==> Confusion:
[[ 967    0    2    2    1    2    4    1    0    1]
 [   2 1121    4    0    4    0    2    2    0    0]
 [   5    0 1019    3    0    1    0    3    0    1]
 [   0    0    8  996    0    4    0    1    0    1]
 [   0    2    3    0  955    0    2    0    0   20]
 [   0    1    1    8    0  875    2    2    0    3]
 [   7    3    6    0    1   15  926    0    0    0]
 [   1    1   22    5    5    0    0  986    0    8]
 [ 541    1   30   45   12  168   77    7    0   93]
 [   6    3    3    1   10    7    0    5    0  974]]

2024-08-23 15:16:28,497 - 
2024-08-23 15:16:28,498 - Log file for this run: /root/Summer_Project/ai8x-training/logs/2024.08.23-151554/2024.08.23-151554.log
